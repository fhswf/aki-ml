{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <IMG SRC=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d5/Fachhochschule_Südwestfalen_20xx_logo.svg/320px-Fachhochschule_Südwestfalen_20xx_logo.svg.png\" WIDTH=250 ALIGN=\"right\">\n",
    "</figure>\n",
    "\n",
    "# Machine Learning\n",
    "### Sommersemester 2025\n",
    "Prof. Dr. Stefan Goetze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines und Hauptkomponentenanalyse\n",
    "\n",
    "Eine *Support Vector Machine* (SVM) ist ein sehr leistungsfähiges und vielseitiges Modell für maschinelles Lernen, mit dem lineare oder nichtlineare Klassifizierung, Regression und sogar Ausreißererkennung durchgeführt werden können. In diesem Notebook werden wir die Umsetzung von SVMs mit Scikit-learn übern. Im Zweiten Teil des Notebooks werden wir die Hauptkomponentenanalyse für die Visualisierung der Daten verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Support Vector Machine (SVM)\n",
    "\n",
    "Support-Vektor-Machines werden vielfach benutzt, weil sie gute Resultate bei relativ geringerem Rechenaufwand liefern. SVMs können sowohl für Regressions- als auch für Klassifikationsaufgaben verwendet werden, wobei die binäre Klassifikation das Haupteinsatzgebiet sind.\n",
    "\n",
    "Das Ziel des SVM Trainings besteht darin, eine Hyperebene in einem $N$-dimensionalen Raum zu finden, die die Datenpunkte eindeutig klassifiziert. $N$ ist dabei gleich der Anzahl der Merkmale.\n",
    "\n",
    "Mögliche Entscheidungsgrenzen | Optimale Entscheidungsgrenze\n",
    ":---: | :---: \n",
    "![](https://docs.opencv.org/4.0.0/separating-lines.png) | ![](https://docs.opencv.org/4.0.0/optimal-hyperplane.png)\n",
    "\n",
    "(Bildquelle: opencv.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/publication/341356165/figure/fig3/AS:962229604216844@1606424823570/Hyperplanes-in-2D-and-3D-space.png)\n",
    "(Bildquelle: Koren, Jurčević, Prasad. 2020)\n",
    "\n",
    "Hyperebenen sind Entscheidungsgrenzen, mit denen die Datenpunkte klassifiziert werden können. Datenpunkte, die auf beide Seiten der Hyperebene fallen, können verschiedenen Klassen zugeordnet werden. Die Dimension der Hyperebene hängt auch von der Anzahl der Merkmale ab. Wenn die Anzahl der Eingabemerkmale 2 beträgt, ist die Hyperebene nur eine Linie. Wenn die Anzahl der Eingabemerkmale 3 beträgt, wird die Hyperebene zu einer zweidimensionalen Ebene im Raum. Für höhere Dimensionen ist die Darstellung der Hyperebenen nicht ohne Weiteres möglich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*QqpnpuTrG9UevIT5kBX3Zg.jpeg)\n",
    "(Bildquelle: Madhu Ramiah)\n",
    "\n",
    "Support Vektoren sind Datenpunkte, die in unmittelbarer Nähe zur Hyperebene liegen und die Position und Ausrichtung der Hyperebene beeinflussen. Mit diesen Support Vektoren maximieren wir den Rand des Klassifikators.\n",
    "\n",
    "Bei der logistischen Regression nehmen wir die Ausgabe der linearen Funktion und transformieren den Wert in den Bereich von [0,1] mithilfe der Sigmoidfunktion. Wenn der transformierte Wert größer als ein Schwellenwert (i.d.R. $0,5$) ist, weisen wir dem Wert das Resultat $1$ zu, andernfalls weisen wir das Resultat $0$ zu. In SVMs nehmen wir die Ausgabe der linearen Funktion und wenn diese Ausgabe größer als 1 ist, identifizieren wir sie es mit der Klasse $1$ und wenn die Ausgabe kleiner als -1 ist, ordnen wir dem Punkt die Klasse $0$ zu. Da die Schwellenwerte in SVM auf 1 und -1 geändert werden, erhalten wir den Wertebereich [- 1,1] als Pufferzone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM mit Scikit-learn\n",
    "\n",
    "In diesem Aufgabenblatt verwenden SVMs für die Vorhersage von Krebsdiagnosen.\n",
    "Die beiden zu unterscheidenden Klassen sind dabei, ob der Krebs eher *gutartig* oder *bösartig* ist.\n",
    "Zur vorhersage ziehen wir insgesamt 30 Merkmale hinzu, u.a. die die folgenden:\n",
    " - Radius (Mittelwert der Abstände vom Zentrum zu den Punkten am Umfang)\n",
    " - Textur (Standardabweichung der Graustufenwerte)\n",
    " - Umfang\n",
    " - Bereich\n",
    " - Glätte (lokale Variation der Radiuslängen)\n",
    " - Kompaktheit (Umfang ^ 2 / Fläche - 1,0)\n",
    " - Konkavität (Schweregrad der konkaven Teile der Kontur)\n",
    " - konkave Punkte (Anzahl der konkaven Teile der Kontur)\n",
    " - Symmetrie\n",
    "\n",
    "\n",
    "Der Datensatz ist mit allen 30 Merkmalen linear trennbar.\n",
    "Insgesamt enthält der Datensatz 569 Datenpunkte, dovon sind 212 bösartig (engl. *malignant*) und 357 gutartig (engl. *benign*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "except:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install seaborn\n",
    "    \n",
    "    import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "col_names = list(cancer.feature_names)\n",
    "col_names.append(u'target')\n",
    "df = pd.DataFrame(np.c_[cancer.data, cancer.target], columns=col_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der Daten\n",
    "\n",
    "Zur Visualisierung der Daten verwenden wir in diesem Arbeitsblatt die *Seaborn* Bibliothek.\n",
    "Seaborn basiert auf Matplotlib und bietet einige *höhere* Methoden für die optisch ansprechende Darstellung von Statistik-Diagrammen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborn.pairplot` ist eine *schönere* Variante der `scatter_matrix` Funktion aus der Pandas Bibliothek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='target', vars=['mean radius', 'mean texture', 'mean perimeter', 'mean area', \n",
    "                                     'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "                                     'mean concave points', 'mean symmetry', 'mean fractal dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df['target'], label = \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einzelne Feature-Paare können auch über einen Skatter-Plot verglichen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x = 'mean area', y = 'mean smoothness', hue = 'target', data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit `seaborn.heatmap` können wir uns die Korrelationsmatrix der Merkmale als Diagramm anzeigen lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10)) \n",
    "sns.heatmap(df.corr(), annot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ein SVM-Modell trainieren\n",
    "\n",
    "Das Trainingsverhalten eines SVM-Klassifizierer (*Support Vector Machine Classifier*, SVC) kann mit verschiedenen Eingaben parametrisiert werden:\n",
    "\n",
    "- `C`: Steuert das Verhältnis zwischen der korrekten Klassifizierung von Trainingsdatenpunkten und einer glatten Entscheidungsgrenze. Ein kleines `C` senkt die Kosten für falsch klassifizierte Punkte. Ein größeres `C` erhöht die Kosten für  Fehlklassifizierungen und zwingt das Modell damit, die Eingabedaten starrer zu beschreiben. Letzeres kann also zu einer Überanpassung führen. Mit einem groß gewählten `C`-Wert wird Ihr Modell also *genauer*, neigt damit aber auch zu Overfitting.\n",
    "- `Gamma`: Steuert, wie weit der Einfluss eines einzelnen Trainingsdatenpunktes reicht. Ein großes Gamma schränkt die Reichweite ein. Damit haben eng zusammen liegende Datenpunkte ein hohes Gewicht. Ein kleines Gamma erhöht die Reichweite. Die Gwichtung von Punkten wird damit weniger Anhängig davon, ob die Punkte gebündelt auftreten oder nicht.\n",
    "- `Poly`: Grad der Polynomkernfunktion beim Polynomiellen Kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 1: Lineare Klassifikation** \n",
    "\n",
    "Verwenden Sie die Klasse `LinearSVC`, um einen linearen SVM-Klassifizierer zu trainiern. Bewerten Sie die Vorhersagegenauigkeit (*Classification Accuricy*) jeweils auf Grundlage der Trainings- und Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2bf15da18aeaaac2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "accuracy_train = None\n",
    "accuracy_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Accuracy (Training): {accuracy_train * 100:.2f}%\")\n",
    "print(f\"Accuracy (Test): {accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 2: Polynomieller Kernel** \n",
    "\n",
    "Verwenden Sie die Klasse `SVC`, um einen polyomiellen SVM-Klassifizierer zu trainiern.\n",
    "Verwenden Sie ein Plynom zweiten Grades, für `gamma` den Wert 0.5, für `coef0` den Wert 1 und für `C` den Wert 5.\n",
    "Bewerten Sie die Vorhersagegenauigkeit (*Classification Accuricy*) jeweils auf Grundlage der Trainings- und Testdaten.\n",
    "\n",
    "- `gamma` wird für nichtlineare SVM verwendet und bestimmt, wie weit der Einfluss jeder Stichprobe ist\n",
    "- Ein Wert von `coef0` ungleich Null führt zu einem nicht-symmetrischen Kernel\n",
    "- Der `C`-Parameter (Regularisierungsparameter) ist der Grad, zu dem das Modell Fehlklassifizierungen akzeptiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-936836c942d0a583",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "accuracy_train = None\n",
    "accuracy_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Accuracy (Training): {accuracy_train * 100:.2f}%\")\n",
    "print(f\"Accuracy (Test): {accuracy_test * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 3: Gaußscher RBF-Kernel** \n",
    "\n",
    "Verwenden Sie die Klasse `SVC`, um einen SVM-Klassifizierer mit RBF (*Radial Basis Function*) Kernel zu trainiern.\n",
    "Für `gamma` setzen Sie den Wert 0.5 ein und für `C` den Wert 0.1.\n",
    "Bewerten Sie die Vorhersagegenauigkeit (*Classification Accuricy*) jeweils auf Grundlage der Trainings- und Testdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-76809ad86f7356a2",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "accuracy_train = None\n",
    "accuracy_test = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(f\"Accuracy (Training): {accuracy_train * 100:.2f}%\")\n",
    "print(f\"Accuracy (Test): {accuracy_test * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenvorverarbeitung für SVMs\n",
    "\n",
    "Mit dem SVM-Algorithmus finden wir Hyperebenen in unserem mehrdimensionalen Datensatz.\n",
    "Dabei vergleichen wir die *Abstände* von Punkten im Raum zur betrachteten Hyperebene.\n",
    "Es ist anschaulich klar, dass es sinnvoll ist, wenn die Abstände der Punkte die gleiche Gewichtung in allen Dimensionen haben.\n",
    "Wenn ein Merkmal in der Größenordnung von Tausenden ist, ein anderes in der Größenordnung von Hundertsteln, passen die Abstände sicher nicht zu zusammen. Ein Verschiebung hat dann leicht auf das eine Merkmal einen zu geringen, und gleichzeitig auf das das andere Merkmal einen zu hohen Einfluss.\n",
    "Es ist daher unbedingt ratsam, die Daten vor dem Trainieren des SVM Modells zu normalisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 4: SVM Klassifizierung mit Normalisierten Daten**\n",
    "\n",
    "Wenden Sie die obigen SVM Klassifikatoren erneut an, nun aber auf Basis der transformierten Daten.\n",
    "Speichern Sie die Accuracy-Scores in den Listen `accuracy_train` und `accuracy_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22df0998610da06d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_linear = SVC(kernel='linear')\n",
    "model_poly   = SVC(kernel='poly', degree=2, gamma=0.5, coef0=1, C=5)\n",
    "model_rbf    = SVC(kernel='rbf', gamma=.5)\n",
    "\n",
    "\n",
    "def evaluate(X_train, X_test, y_train, y_test, model, label=''):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "evaluate(X_train, X_test, y_train, y_test, model_linear, 'Linearer')\n",
    "evaluate(X_train, X_test, y_train, y_test, model_poly, 'Polynomieller')\n",
    "evaluate(X_train, X_test, y_train, y_test, model_rbf, 'RBF')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Hyperparameteroptimierung\n",
    "\n",
    "Ein üblicher Ansatz, um gute Hyperparameterwerte zu finden, ist die Verwendung der Rastersuche (engl. *grid search*).\n",
    "Dabei wird für alle Hyperparameterwer eine Reihen von Vorgabewerten definiert, die der Suchalgorithmus dann durchläuft und alle möglichen Kombinationen dieser Parameter durchprobiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100],\n",
    "              'coef0': [0.0, 0.1, 0.5, 1, 2, 10],\n",
    "              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n",
    "              'kernel': ['rbf', 'poly', 'linear']} \n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid.best_params_\n",
    "print(f\"Beste Kombination von Parametern: {best_params}\")\n",
    "\n",
    "svm_opt = SVC(**best_params)\n",
    "evaluate(X_train, X_test, y_train, y_test, svm_opt, 'Best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausreißererkennung mir SVMs\n",
    "\n",
    "SVMs können nicht nur zur Klassifikation, sondern auch (für sog. *Einklassenprobleme*) zum erkennen von Anomalien eingesetzt werden.\n",
    "In diesem Fall wird das Modell so trainiert, dass es lernt, was \"normal\" ist.\n",
    "Daten, die außerhalb des *Normalen* liegen, werden als *anomal* eingestuft.\n",
    "\n",
    "Das Modell `OneClassSVM` aus Scikit-learn kann für die Erkennung von Ausreißern verwendet werden.\n",
    "`OneClassSVM` verwendet standardmäßig einen Gauß-Kernel (`kernel='rbf'`) und erfordert eine Feinabstimmung des Hyperparameters `nu` um Overfitting zu verhindern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden Code-Beispiel generieren wir einen *Blob* von Daten und wenden ein `OneClassSVM`-Modell darauf an.\n",
    "Durch die Wahl des Parameters `nu=0.03` sollen 3% der Daten als Außreißer identifiziert werden.\n",
    "Außerdem liefert die Standardeinstellung für den Streuungsparameter `gamma` schlechte Ergebnisse, weswegen wir die Policy von `scale` auf `auto` umstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, _ = make_blobs(n_samples=200, centers=1, cluster_std=.3, center_box=(8, 8))\n",
    "\n",
    "# OneClassSVM Modell initialisieren\n",
    "model = OneClassSVM(kernel = 'rbf', gamma='auto', nu = 0.03).fit(X)\n",
    "\n",
    "# Vorhersage\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "outlier_index = np.where(y_pred == -1) \n",
    "outlier_values = X[outlier_index]\n",
    "\n",
    "# Streudiagramm mit Ausreißern\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(outlier_values[:,0], outlier_values[:,1], c = \"r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung mit PCA\n",
    "\n",
    "Die Hauptkomponentenanalyse (engl. *Principal Component Analysis*, PCA) ist ein Verfahren, das häufig dazu eingesetzt wird hochdimensionale Datensätze zu vereinfachen oder zu visualisieren.\n",
    "Dazu werden die Merkmale durch eine geringere Anzahzahl möglichst aussagekräftiger Linearkombinationen der Merkmale (den sogenannten Hauptkomponenten) angenähert.\n",
    "\n",
    "In der Abbildung unten sehen wir zwei Merkmale (*Feature 1* und *2*), sowie die Zielgröße, die über die Farbliche Codierung der Punkte dargestellt wird.\n",
    "Die erste Hauptkomponente liegt ca. entlang der Diagonalen von links oben nach rechts unten im Diagramm. Mathematisch gesehen, weisen die Punkte entlang dieser Achse die größte Streuung auf. Die zweite Hauptkomponente liegt orthogonal zur ersten. Hierauf Weisen die Punkte eine geringere Streuung auf.\n",
    "\n",
    "Legt man nun die die Hauptkomponenten entlang der $x$- und $y$-Achse des Koordinatensystems an, gelangt man zur Abbildung oben rechts. Entfernt man aus diesen Daten die Werte entlang der zweiten Hauptkomponente, kommt man zur Abbildung unten links.\n",
    "Durch eine Rücktransformation mit der ersten Hauptkomponente entsteht die Abbildung unten rechts. Man sieht, dass diese vereinfachten Daten (statt zwei Variablen, besteht dieser Datensatz nur aus einer einer Variablen) den AUsgangsdatensatz recht gut abbilden.\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*zJagNPoOBSTfD7_4.png)\n",
    "(Quelle: A. Müller, S. Guido)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits angesprochen, ist es schwierig, hochdimensionale Daten zu visualisieren.\n",
    "Mithilfe ist der PCA ist es nun möglich, die ersten beiden Hauptkomponenten eines Datensatzes zu ermitteln, und die auf diese beiden Hauptkomponenten transformierten Daten in einem zweidimensionalen Koordinatensystem als Streudiagramm darzustellen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 5: Standardisierung**\n",
    "\n",
    "Auch bei der PCA ist ein Normalisieren der Daten notwendig.\n",
    "Verwenden Sie den `StandardScaler` aus *Scikit-learn*, um die Daten zu Standardisieren. Die transformierten Merkmale (vor Verwendung der PCA) eine einheitliche Standardabweichung von 1 ausweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f93da85e45c56f4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_transform = None\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df.target\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-397379addf90e6ed",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.sum((np.std(X_train, axis=0)-1)**2)<0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die PCA in *Scikil-learn* funktioniert wie andere Transomatoren (z.B. `StandardScaler` oder `MinMaxScaler`) auch.\n",
    "Bei der Initialisierung des der `PCA`-Instanz kann man die Anzahl der Hauptkomponenten angeben, die bei der Transformation beibehalten werden sollen.\n",
    "\n",
    "**Aufgabe 6: PCA durchführen**\n",
    "Verwenden die Klasse PCA aus *Scikil-learn*, um den Datensatz von ursprünglich 30 auf 3 Merkmale zu reduzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bc8b2363c049e5da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b5eb8a42a9c19022",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert X_test.shape[1]==X_train.shape[1]==3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir die Daten in der Ebene visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[:,0],X_train[:,1],c=y_train,cmap='plasma')\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beispiel: Eigenfaces\n",
    "\n",
    "Am Beispiel der *Eigenfaces* wollen wir untersuchen, wie mittels der Hauptkomponentenanalyse Daten in der Praxis reduziert werden können, ohne allzuviel Information dabei zu verlieren. Im Grunde kann man das Verfahren so acu als (verlustbehaftete) Komression auffassen.\n",
    "\n",
    "Der [*Labeled Faces in the Wild*](http://vis-www.cs.umass.edu/lfw/) Datensatz enthält Portraitbilder bekannter Persönlichkeiten und ist über die Scikit-Learn Bibliothek verfügbar.\n",
    "Wir wollen diesen Datensatz verwenden, um Bilddaten mittes der Hauptkomponentenanalyse zu komprimieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=50)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten wir nun die Hauptkomponenten der einzelnen Bilder.\n",
    "Da es sich um einen großen Datensatz handelt, werden wir das Verfaheren `RandomizedPCA` verwenden - es enthält eine randomisierte Methode, um die ersten $N$ Hauptkomponenten viel schneller als der Standard-PCA-Schätzer zu approximieren und ist daher sehr nützlich für hochdimensionale Daten (hier eine Dimensionalität von fast 3.000). Wir können nun die ersten 150 Komponenten berechnen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(150, svd_solver='randomized')\n",
    "pca.fit(faces.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Fall kann es interessant sein, die Bilder zu visualisieren, die mit den ersten paar Hauptkomponenten verbunden sind (diese Komponenten werden technisch als \"Eigenvektoren\" bezeichnet, daher werden diese Art von Bildern oft als \"Eigenfaces\" bezeichnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "fig, axes = plt.subplots(3, 8, figsize=(15, 8),\n",
    "                         subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                         gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ergebnisse geben uns Einblicke, wie sich die Bilder unterscheiden: Zum Beispiel scheinen die ersten paar Eigenfaces (oben links) mit dem Lichteinfallswinkel auf das Gesicht verbunden zu sein, während andere Hauptvektoren bestimmte Merkmale wie Augen, Nasen und Lippen auswählen.\n",
    "Lassen Sie uns einen Blick auf die kumulative Varianz dieser Komponenten werfen, um zu sehen, wie viel der Dateninformation die Projektion beibehält:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir sehen, dass diese 150 Komponenten etwas mehr als 90% der Varianz ausmachen.\n",
    "Das würde uns glauben lassen, dass wir mit diesen 150 Komponenten die meisten wesentlichen Merkmale der Daten wiederherstellen würden.\n",
    "Um dies konkreter zu machen, können wir die Eingangsbilder mit den aus diesen 150 Komponenten rekonstruierten Bildern vergleichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the components and projected faces\n",
    "pca = PCA(150, svd_solver='randomized').fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(15, 4),\n",
    "                       subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                       gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "ids = random.sample(range(len(faces.data)), 10)\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[ids[i]].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[ids[i]].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('150-dim\\nreconstruction');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die obere Reihe zeigt hier die Eingangsbilder, während die untere Reihe die Rekonstruktion der Bilder aus nur 150 der ~3.000 ursprünglichen Merkmale zeigt. Obwohl die Dimensionalität der Daten um fast einen Faktor von 20 reduziert wird, enthalten die projizierten Bilder genügend Informationen, um die Personen auf den Bildern erkennen zu können.\n",
    "Das bedeutet, dass ein Klassifikationsalgorithmus nun mit 150-dimensionale Daten statt auf 3.000-dimensionale Daten trainiert werden kann, was je nach gewähltem Algorithmus zu einer wesentlich effizienteren Klassifizierung führen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Referenzen:\n",
    "\n",
    "- Fares Sayah, [*Support Vector Machine & PCA Tutorial for Beginner*](https://www.kaggle.com/faressayah/support-vector-machine-pca-tutorial-for-beginner), Eintrag auf Kaggle.com, 2021 (abgerufen am 14.05.2021)\n",
    "- Rohith Gandhi, [*Support Vector Machine - Introduction to Machine Learning Algorithms*](https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47)\n",
    "Artikel auf towardsdatascience.com, 2018 (abgerufen am 14.05.2021)\n",
    "- [*Scikit-learn - Support Vector Machines*](https://scikit-learn.org/stable/modules/svm.html#svm-kernels)\n",
    "- [*Scikit-learn - Dekomposition mit PCA*](http://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "- [Mahbubul Alam, *Support Vector Machine (SVM) for Anomaly Detection*, Towards Data Science, 2020](https://towardsdatascience.com/support-vector-machine-svm-for-anomaly-detection-73a8d676c331)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
